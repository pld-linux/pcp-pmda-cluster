diff -Nur pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/ib/help pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/ib/help
--- pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/ib/help	1970-01-01 01:00:00.000000000 +0100
+++ pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/ib/help	2012-12-26 19:59:50.082528880 +0100
@@ -0,0 +1,166 @@
+#
+# Copyright (c) 2007,2008 Silicon Graphics, Inc.  All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it
+# under the terms of the GNU General Public License as published by the
+# Free Software Foundation; either version 2 of the License, or (at your
+# option) any later version.
+#
+# This program is distributed in the hope that it will be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+# or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+# for more details.
+#
+# You should have received a copy of the GNU General Public License along
+# with this program; if not, write to the Free Software Foundation, Inc.,
+# 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+
+@ infiniband.hca.type Node type
+Node type: channel adapter (CA), switch, router etc
+@ infiniband.hca.ca_type HCA type
+HCA type, e.g. MT23108,
+@ infiniband.hca.numports Number of ports on HCA
+Number of ports on HCA
+@ infiniband.hca.fw_ver Version of HCA firmware
+Version of HCA firmware
+@ infiniband.hca.hw_ver Version of HCA hardware
+Version of HCA hardware
+@ infiniband.hca.node_guid Node's Global Unique Identifier
+Node's Global Unique Identifier - 64 bit integer to refer to the node
+@ infiniband.hca.system_guid System's Global Unique Identifier
+System's Global Unique Identifier - 64 bit integer to refer to the system
+@ infiniband.port.guid Port's Global Unique Identifier
+Port's Global Unique Identifier - 64 bit integer to refer to the port
+@ infiniband.port.gid_prefix GID prefix
+GID prefix, assigned by subnet manager
+@ infiniband.port.lid Port's Local Identifier
+Port's Local Identifier, assigned by subnet manager
+@ infiniband.port.state Port's state
+Port's state - can be Active, Down, NoChange, Armed or Initialize
+@ infiniband.port.phystate Port's physical state
+Port's physical state
+@ infiniband.port.rate Port's Data Rate
+Port's Data Rate: 2, 5, 10 or 20 Gbps
+@ infiniband.port.capabilities Port's capabilities
+Port's capabilities.
+@ infiniband.port.linkspeed Base link speed of the port.
+This is a string which represents the base link speed of the port.
+Multiplying link speed by link width gives port's data rate.
+@ infiniband.port.linkwidth Port's link width.
+Number of bi-directional Infiniband links active on the port.
+Also known as X-factor, as in 1X, 4X, 12X.
+@ infiniband.port.in.bytes Bytes received
+Counter of data octets received on all VLs at the port. This 
+includes all octets between (and not including) the start of 
+packet delimiter and the VCRC, and may include packets containing errors.
+It excludes all link packets. 
+
+This counter is implemented by sampling underlying saturating PortRcvData 
+counter. When a value of saturated counter reaches predefined threshold,
+the counter is reset after its value is copied into internal state.
+
+@ infiniband.port.in.packets Packets received
+Counter of data packets received on all VLs at the port. This 
+may include packets containing errors but excludes all link packets. 
+
+@ infiniband.port.in.errors.drop Packets dropped due to errors
+Number of packets received on the port that were discarded because they 
+could not be forwarded by the switch relay due to DLID mapping, VL mapping
+or looping. Implemented by sampling 16 bit PortRcvSwitchRelayErrors 
+counter.
+
+@ infiniband.port.in.errors.filter Packets filtered out
+Number of packets received by the port that were discarded because
+it was a raw packet and FilterRawInbound is enabled or because 
+PartitionEnforcementInbound is enabled and packet failed partition 
+key check or IP version check. Implemented by sampling  8 bit 
+PortRcvConstraintErrors counter.
+
+@ infiniband.port.in.errors.local Packets with errors
+Counter of packets containing local physical errors, malformed data or 
+link packets or packets discarded due to buffer overrun. Implemented by
+sampling 16 bit PortRcvErrors counter.
+
+@ infiniband.port.in.errors.remote Packets with EBP delimiter.
+Number of packets marked with End Bad Packet delimited received by
+the port. Implemented by sampling 16 bit PortRcvRemotePhysicalerrors 
+counter.
+
+@ infiniband.port.out.bytes Bytes transmitted
+Counter of data octets, transmitted on all VLs from the port. This 
+includes all octets between (and not including) the start of 
+packet delimiter and the VCRC, and may include packets containing errors.
+It excludes all link packets. 
+
+This counter is implemented by sampling underlying saturating PortXmtData 
+counter. When a value of saturated counter reaches predefined threshold,
+the counter is reset after its value is copied into internal state.
+
+@ infiniband.port.out.packets Packets transmitted
+Counter of data packets transmitted on all VLs from the port. This 
+may include packets containing errors but excludes all link packets. 
+
+@ infiniband.port.out.errors.drop Packets dropped without transmitting
+Number of outbound packets which were droped because port is down
+or congested. Implemented by sampling 16 bit PortXmtDiscard counter.
+
+@ infiniband.port.out.errors.filter Packets filtered out before transmitting
+Number of packets not transmitted by the port because
+it was a raw packet and FilterRawInbound is enabled or because 
+PartitionEnforcementInbound is enabled and packet failed partition 
+key check or IP version check. Implemented by sampling  8 bit 
+PortXmitConstraintErrors counter.
+
+@ infiniband.port.total.bytes Bytes transmitted and received
+Cumulative value of infiniband.port.in.bytes and 
+infiniband.port.out.bytes, provided for convenience.
+
+@ infiniband.port.total.packets Packets transmitted and received
+Cumulative value of infiniband.port.in.packets and 
+infiniband.port.out.packets, provided for convenience.
+
+@ infiniband.port.total.errors.drop Packet dropped
+Cumulative counter of infiniband.port.in.errors.drop and 
+infiniband.out.errors.drops.
+
+@ infiniband.port.total.errors.filter Packet filtered out
+Cumulative counter of infiniband.port.in.errors.filter and 
+infiniband.out.errors.filter.
+
+@ infiniband.port.total.errors.link Link downed
+Number of times Port Training state machine has failed to 
+complete link recovery process and downed the link. Implemented by 
+sampling 8 bit LinkDownedCounter.
+
+@ infiniband.port.total.errors.recover Successful recoveries
+Number of times Port Training state machine has managed successfully
+complete link recovery process. Implemented by sampling 8 bit 
+LinkErrorRecoveryCounter.
+
+@ infiniband.port.total.errors.integrity Excessive local physical errors
+Number of times the count of local physical errors exceeded the threshold.
+Implemented by sampling 4 bit LocalLinkIntegrityErrors counter.
+
+@ infiniband.port.total.errors.vl15 Dropped packets to VL15
+Number of times packets to VL15 (management virtual line) was dropped
+due to resource limitations. Implemented by sampling 16 bit VL15Dropped
+counter.
+
+@ infiniband.port.total.errors.overrun Excessive Buffer Overruns
+The number of times buffer overrun errors had persisted over multiple 
+flow control update times.  Implemented by sampling 4 bit 
+ExcessiveBufferOverrun counter.
+
+@ infiniband.port.total.errors.symbol Total number of minor link errors
+Total number of minor link errors detected on one or more physical lines.
+Implemented by sampling 16 bit SymbolErrorCounter.
+
+@ infiniband.control.query_timeout Timeout for MAD perquery
+Timeout in milliseconds for MAD rpcs. Default value is 1000 milliseconds.
+Timeout can be set per port.
+
+@ infiniband.control.hiwat Counter threshold values
+Threshold values for each MAD performance counter. Due to saturating
+nature of the counters they're reset when value of a particular counter
+gets above a threshold. Setting threshold to the maximum value disables
+the reset mechanism.
diff -Nur pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/ib/root pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/ib/root
--- pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/ib/root	1970-01-01 01:00:00.000000000 +0100
+++ pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/ib/root	2012-12-26 19:59:40.635862412 +0100
@@ -0,0 +1,9 @@
+/*
+ * Generated code, do not edit!
+ */
+#define PCP_PMNS
+#include "domain.h"
+
+root { infiniband }
+
+#include "pmns"
diff -Nur pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/linux/help pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/linux/help
--- pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/linux/help	1970-01-01 01:00:00.000000000 +0100
+++ pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/linux/help	2012-12-26 19:59:22.635862787 +0100
@@ -0,0 +1,1481 @@
+#
+# Copyright (c) 2000,2004-2008 Silicon Graphics, Inc.  All Rights Reserved.
+# Portions Copyright (c) International Business Machines Corp., 2002
+# Portions Copyright (c) 2007-2009 Aconex.  All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it
+# under the terms of the GNU General Public License as published by the
+# Free Software Foundation; either version 2 of the License, or (at your
+# option) any later version.
+#
+# This program is distributed in the hope that it will be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+# or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+# for more details.
+#
+# Linux PMDA help file in the ASCII format
+#
+# lines beginning with a # are ignored
+# lines beginning @ introduce a new entry of the form
+#  @ metric_name oneline-text
+#  help test goes
+#  here over multiple lines
+#  ...
+#
+# the metric_name is decoded against the default PMNS -- as a special case,
+# a name of the form NNN.MM (for numeric NNN and MM) is interpreted as an
+# instance domain identification, and the text describes the instance domain
+#
+# blank lines before the @ line are ignored
+#
+@ kernel.uname.release release level of the running kernel
+Release level of the running kernel as reported via the release[]
+value returned from uname(2) or uname -r.
+
+See also pmda.uname.
+
+@ kernel.uname.version version level (build number) and build date of the running kernel
+Version level of the running kernel as reported by the version[]
+value returned from uname(2) or uname -v.  Usually a build number
+followed by a build date.
+
+See also pmda.uname.
+
+@ kernel.uname.sysname name of the implementation of the operating system
+Name of the implementation of the running operating system as reported
+by the sysname[] value returned from uname(2) or uname -s.  Usually
+"Linux".
+
+See also pmda.uname.
+
+@ kernel.uname.machine name of the hardware type the system is running on
+Name of the hardware type the system is running on as reported by the machine[]
+value returned from uname(2) or uname -m, e.g. "i686".
+
+See also pmda.uname.
+
+@ kernel.uname.nodename host name of this node on the network
+Name of this node on the network as reported by the nodename[]
+value returned from uname(2) or uname -n.  Usually a synonym for
+the host name.
+
+See also pmda.uname.
+
+@ kernel.uname.distro Linux distribution name
+The Linux distribution name, as determined by a number of heuristics.
+For example:
++ on Fedora, the contents of /etc/fedora-release
++ on RedHat, the contents of /etc/redhat-release
+
+@ kernel.percpu.cpu.user percpu user CPU time metric from /proc/stat
+@ kernel.percpu.cpu.nice percpu nice user CPU time metric from /proc/stat
+@ kernel.percpu.cpu.sys percpu sys CPU time metric from /proc/stat
+@ kernel.percpu.cpu.idle percpu idle CPU time metric from /proc/stat
+@ kernel.percpu.cpu.wait.total percpu wait CPU time
+Per-CPU I/O wait CPU time - time spent with outstanding I/O requests.
+
+@ kernel.percpu.cpu.intr percpu interrupt CPU time 
+Total time spent processing interrupts on each CPU (this includes
+both soft and hard interrupt processing time).
+
+@ kernel.percpu.cpu.irq.soft percpu soft interrupt CPU time 
+Per-CPU soft interrupt CPU time (defered interrupt handling code,
+not run in the initial interrupt handler).
+
+@ kernel.percpu.cpu.irq.hard percpu hard interrupt CPU time 
+Per-CPU soft interrupt CPU time ("hard" interrupt handling code
+is the code run directly on receipt of the initial hardware
+interrupt, and does not include "soft" interrupt handling code
+which is defered until later).
+
+@ kernel.percpu.cpu.steal percpu CPU steal time 
+Per-CPU time when the CPU had a runnable process, but the hypervisor
+(virtualisation layer) chose to run something else instead.
+
+@ kernel.percpu.cpu.guest percpu guest CPU time
+Per-CPU time spent running (virtual) guest operating systems.
+
+@ kernel.all.interrupts.errors interrupt error count from /proc/interrupts
+This is a global counter (normally converted to a count/second)
+for any and all errors that occur while handling interrupts.
+
+@ disk.dev.read per-disk read operations
+Cumulative number of disk read operations since system boot time (subject
+to counter wrap).
+
+If the "sard" patch has been applied, the values are obtained from
+/proc/partitions, otherwise they are obtained from /proc/stat. In the
+latter case, (a) some metric values may be shared by multiple disks
+(this may happen if both scsi and ide disks are present in the system),
+in which case the disk instance name will show multiple device names
+separated by "+", and (b) metric values may not be available at all for
+some disks - this only happens if there are more than four disk devices
+installed in the system, and is the unfortunate end result of a
+limitation in the kernel's I/O accounting.
+
+@ disk.dev.write per-disk write operations
+Cumulative number of disk write operations since system boot time (subject
+to counter wrap).
+
+If the "sard" patch has been applied, the values are obtained from
+/proc/partitions, otherwise they are obtained from /proc/stat. In the
+latter case, (a) some metric values may be shared by multiple disks
+(this may happen if both scsi and ide disks are present in the system),
+in which case the disk instance name will show multiple device names
+separated by "+", and (b) metric values may not be available at all for
+some disks - this only happens if there are more than four disk devices
+installed in the system, and is the unfortunate end result of a
+limitation in the kernel's I/O accounting.
+
+@ disk.dev.total per-disk total (read+write) operations
+Cumulative number of disk read and write operations since system boot
+time (subject to counter wrap).
+
+If the "sard" patch has been applied, the values are obtained from
+/proc/partitions, otherwise they are obtained from /proc/stat. In the
+latter case, (a) some metric values may be shared by multiple disks
+(this may happen if both scsi and ide disks are present in the system),
+in which case the disk instance name will show multiple device names
+separated by "+", and (b) metric values may not be available at all for
+some disks - this only happens if there are more than four disk devices
+installed in the system, and is the unfortunate end result of a
+limitation in the kernel's I/O accounting.
+
+@ disk.dev.blkread per-disk block read operations
+Cumulative number of disk block read operations since system boot time
+(subject to counter wrap).
+
+If the "sard" patch has been applied, the values are obtained from
+/proc/partitions, otherwise they are obtained from /proc/stat. In the
+latter case, (a) some metric values may be shared by multiple disks
+(this may happen if both scsi and ide disks are present in the system),
+in which case the disk instance name will show multiple device names
+separated by "+", and (b) metric values may not be available at all for
+some disks - this only happens if there are more than four disk devices
+installed in the system, and is the unfortunate end result of a
+limitation in the kernel's I/O accounting.
+
+@ disk.dev.blkwrite per-disk block write operations
+Cumulative number of disk block write operations since system boot time
+(subject to counter wrap).
+
+If the "sard" patch has been applied, the values are obtained from
+/proc/partitions, otherwise they are obtained from /proc/stat. In the
+latter case, (a) some metric values may be shared by multiple disks
+(this may happen if both scsi and ide disks are present in the system),
+in which case the disk instance name will show multiple device names
+separated by "+", and (b) metric values may not be available at all for
+some disks - this only happens if there are more than four disk devices
+installed in the system, and is the unfortunate end result of a
+limitation in the kernel's I/O accounting.
+
+@ disk.dev.blktotal per-disk total (read+write) block operations
+Cumulative number of disk block read and write operations since system
+boot time (subject to counter wrap).
+
+If the "sard" patch has been applied, the values are obtained from
+/proc/partitions, otherwise they are obtained from /proc/stat. In the
+latter case, (a) some metric values may be shared by multiple disks
+(this may happen if both scsi and ide disks are present in the system),
+in which case the disk instance name will show multiple device names
+separated by "+", and (b) metric values may not be available at all for
+some disks - this only happens if there are more than four disk devices
+installed in the system, and is the unfortunate end result of a
+limitation in the kernel's I/O accounting.
+
+@ disk.dev.read_bytes per-disk count of bytes read
+This metric is not available if the "sard" patch has not been applied.
+See disk.dev.read for further details.
+
+@ disk.dev.write_bytes per-disk count of bytes written
+This metric is not available if the "sard" patch has not been applied.
+See disk.dev.write for further details.
+
+@ disk.dev.total_bytes per-disk count of total bytes read and written
+This metric is not available if the "sard" patch has not been applied.
+See disk.dev.total for further details.
+
+@ disk.dev.scheduler per-disk I/O scheduler
+The name of the I/O scheduler in use for each device.  The scheduler
+is part of the block layer in the kernel, and attempts to optimise the
+I/O submission patterns using various techniques (typically, sorting
+and merging adjacent requests into larger ones to reduce seek activity,
+but certainly not limited to that).
+
+@ disk.dev.avactive per-disk count of active time
+When converted to a rate, this metric represents the average utilization of
+the disk during the sampling interval.  A value of 0.5 (or 50%) means the
+disk was active (i.e. busy) half the time.
+
+@ disk.dev.read_rawactive per-disk raw count of active read time
+When converted to a rate, this metric represents the raw utilization of
+the disk during the sampling interval as a result of reads.  Accounting for
+this metric is only done on I/O completion and can thus result in more than a
+second's worth of IO being accounted for within any one second, leading to
+>100% utilisation.  It is suitable mainly for use in calculations with other
+metrics, e.g. mirroring the results from existing performance tools:
+
+ iostat.dev.r_await = delta(disk.dev.read_rawactive) / delta(disk.dev.read)
+
+@ disk.dev.write_rawactive per-disk raw count of active write time
+When converted to a rate, this metric represents the raw utilization of
+the disk during the sampling interval as a result of writes.  Accounting for
+this metric is only done on I/O completion and can thus result in more than a
+second's worth of IO being accounted for within any one second, leading to
+>100% utilisation.  It is suitable mainly for use in calculations with other
+metrics, e.g. mirroring the results from existing performance tools:
+
+ iostat.dev.w_await = delta(disk.dev.write_rawactive) / delta(disk.dev.write)
+
+@ disk.dev.aveq per-disk time averaged count of request queue length
+When converted to a rate, this metric represents the time averaged disk
+request queue length during the sampling interval.  A value of 2.5 (or 250%)
+represents a time averaged queue length of 2.5 requests during the sampling
+interval.
+
+@ disk.dev.read_merge per-disk count of merged read requests
+Count of read requests that were merged with an already queued read request.
+
+@ disk.dev.write_merge per-disk count of merged write requests
+Count of write requests that were merged with an already queued write request.
+
+@ disk.all.read_merge total count of merged read requests, summed for all disks
+Total count of read requests that were merged with an already queued read request.
+
+@ disk.all.write_merge total count of merged write requests, summed for all disks
+Total count of write requests that were merged with an already queued write request.
+
+@ disk.all.avactive total count of active time, summed for all disks
+When converted to a rate, this metric represents the average utilization of
+all disks during the sampling interval.  A value of 0.25 (or 25%) means that
+on average every disk was active (i.e. busy) one quarter of the time.
+
+@ disk.all.read_rawactive raw count of active read time, summed for all disks
+When converted to a rate, this metric represents the raw utilization of all
+disks during the sampling interval due to read requests.  The accounting for
+this metric is only done on I/O completion and can thus result in more than a
+second's worth of IO being accounted for within any one second, leading to
+>100% utilisation.  It is suitable mainly for use in calculations with other
+metrics, e.g. mirroring the results from existing performance tools:
+
+ iostat.all.r_await = delta(disk.all.read_rawactive) / delta(disk.all.read)
+
+@ disk.all.write_rawactive raw count of active write time, summed for all disks
+When converted to a rate, this metric represents the raw utilization of all
+disks during the sampling interval due to write requests.  The accounting for
+this metric is only done on I/O completion and can thus result in more than a
+second's worth of IO being accounted for within any one second, leading to
+>100% utilisation.  It is suitable mainly for use in calculations with other
+metrics, e.g. mirroring the result from existing performance tools:
+
+ iostat.all.w_await = delta(disk.all.write_rawactive) / delta(disk.all.write)
+
+@ disk.all.aveq total time averaged count of request queue length, summed for all disks
+When converted to a rate, this metric represents the average across all disks
+of the time averaged request queue length during the sampling interval.  A
+value of 1.5 (or 150%) suggests that (on average) each all disk experienced a
+time averaged queue length of 1.5 requests during the sampling interval.
+
+@ disk.all.read total read operations, summed for all disks
+Cumulative number of disk read operations since system boot time
+(subject to counter wrap), summed over all disk devices.
+
+@ disk.all.write total write operations, summed for all disks
+Cumulative number of disk read operations since system boot time
+(subject to counter wrap), summed over all disk devices.
+
+@ disk.all.total total (read+write) operations, summed for all disks
+Cumulative number of disk read and write operations since system boot
+time (subject to counter wrap), summed over all disk devices.
+
+@ disk.all.blkread block read operations, summed for all disks
+Cumulative number of disk block read operations since system boot time
+(subject to counter wrap), summed over all disk devices.
+
+@ disk.all.blkwrite block write operations, summed for all disks
+Cumulative number of disk block write operations since system boot time
+(subject to counter wrap), summed over all disk devices.
+
+@ disk.all.blktotal total (read+write) block operations, summed for all disks
+Cumulative number of disk block read and write operations since system
+boot time (subject to counter wrap), summed over all disk devices.
+
+@ disk.partitions.read read operations metric from /proc/partitions
+Cumulative number of disk read operations since system boot time
+(subject to counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.all.read_bytes count of bytes read for all disk devices
+This metric is not available if the "sard" patch has not been applied.
+See disk.all.read for further details.
+
+@ disk.all.write_bytes count of bytes written for all disk devices
+This metric is not available if the "sard" patch has not been applied.
+See disk.all.write for further details.
+
+@ disk.all.total_bytes total count of bytes read and written for all disk devices
+This metric is not available if the "sard" patch has not been applied.
+See disk.all.total for further details.
+
+@ disk.partitions.write write operations metric from /proc/partitions
+Cumulative number of disk write operations since system boot time
+(subject to counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.partitions.total total (read+write) I/O operations metric from /proc/partitions
+Cumulative number of disk read and write operations since system boot
+time (subject to counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.partitions.blkread block read operations metric from /proc/partitions
+Cumulative number of disk block read operations since system boot time
+(subject to counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.partitions.blkwrite block write operations metric from /proc/partitions
+Cumulative number of disk block write operations since system boot time
+(subject to counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.partitions.blktotal total (read+write) block operations metric from /proc/partitions
+Cumulative number of disk block read and write operations since system
+boot time (subject to counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.partitions.read_bytes number of bytes read, from /proc/partitions
+Cumulative number of bytes read since system boot time (subject to
+counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.partitions.write_bytes number of bytes written, from /proc/partitions
+Cumulative number of bytes written since system boot time (subject to
+counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ disk.partitions.total_bytes total number of bytes read and written, from /proc/partitions
+Cumulative number of bytes read and written since system boot time
+(subject to counter wrap) for individual disk partitions.
+
+If the "sard" patch has not been applied, the values for this metric
+are unavailable.
+
+@ swap.pagesin pages read from swap devices due to demand for physical memory
+@ swap.pagesout pages written to swap devices due to demand for physical memory
+@ swap.in number of swap in operations
+@ swap.out number of swap out operations
+@ kernel.all.pswitch context switches metric from /proc/stat
+@ kernel.all.sysfork fork rate metric from /proc/stat
+@ kernel.all.intr intrrupt rate metric from /proc/stat
+The value is the first value from the intr field in /proc/stat,
+which is a counter of the total number of interrupts processed.
+The value is normally converted to a rate (count/second).
+This counter usually increases by at least HZ/second,
+i.e. the clock interrupt rate, wehich is usually 100/second.
+
+See also kernel.percpu.interrupts to get a breakdown
+of interrupt rates by interrupt type and which CPU
+processed each one.
+
+@ mem.physmem total system memory metric reported by /proc/meminfo
+The value of this metric corresponds to the "MemTotal" field
+reported by /proc/meminfo. Note that this does not necessarily
+correspond to actual installed physical memory - there may
+be areas of the physical address space mapped as ROM in
+various peripheral devices and the bios may be mirroring
+certain ROMs in RAM.
+@ mem.freemem free system memory metric from /proc/meminfo
+@ mem.util.used used memory metric from /proc/meminfo
+Used memory is the difference between mem.physmem and mem.freemem.
+@ mem.util.free free memory metric from /proc/meminfo
+Alias for mem.freemem.
+@ mem.util.shared shared memory metric from /proc/meminfo
+Shared memory metric. Currently always zero on Linux 2.4 kernels
+and has been removed from 2.6 kernels.
+@ mem.util.bufmem I/O buffers metric from /proc/meminfo
+Memory allocated for buffer_heads.
+@ mem.util.cached page cache metric from /proc/meminfo
+Memory used by the page cache, including buffered file data.
+This is in-memory cache for files read from the disk (the pagecache)
+but doesn't include SwapCached.
+@ mem.util.other unaccounted memory
+Memory that is not free (i.e. has been referenced) and is not cached.
+mem.physmem - mem.util.free - mem.util.cached - mem.util.buffers
+@ mem.util.active Kbytes on the active page list (recently referenced pages)
+Memory that has been used more recently and usually not reclaimed unless
+absolutely necessary.
+@ mem.util.inactive Kbytes on the inactive page list (candidates for discarding)
+Memory which has been less recently used.  It is more eligible to be
+reclaimed for other purposes
+@ mem.util.swapCached Kbytes in swap cache, from /proc/meminfo
+Memory that once was swapped out, is swapped back in but still also
+is in the swapfile (if memory is needed it doesn't need to be swapped
+out AGAIN because it is already in the swapfile. This saves I/O)
+@ mem.util.highTotal Kbytes in high memory, from /proc/meminfo
+This is apparently an i386 specific metric, and seems to be always zero
+on ia64 architecture (and possibly others). On i386 arch (at least),
+highmem is all memory above ~860MB of physical memory. Highmem areas
+are for use by userspace programs, or for the pagecache. The kernel
+must use tricks to access this memory, making it slower to access
+than lowmem.
+@ mem.util.highFree Kbytes free high memory, from /proc/meminfo
+See mem.util.highTotal. Not used on ia64 arch (and possibly others).
+@ mem.util.lowTotal Kbytes in low memory total, from /proc/meminfo
+Lowmem is memory which can be used for everything that highmem can be
+used for, but it is also availble for the kernel's use for its own
+data structures. Among many other things, it is where everything
+from the Slab is allocated.  Bad things happen when you're out of lowmem.
+(this may only be true on i386 architectures).
+@ mem.util.lowFree Kbytes free low memory, from /proc/meminfo
+See mem.util.lowTotal
+@ mem.util.swapTotal Kbytes swap, from /proc/meminfo
+total amount of swap space available
+@ mem.util.swapFree Kbytes free swap, from /proc/meminfo
+Memory which has been evicted from RAM, and is temporarily on the disk
+@ mem.util.dirty Kbytes in dirty pages, from /proc/meminfo
+Memory which is waiting to get written back to the disk
+@ mem.util.writeback Kbytes in writeback pages, from /proc/meminfo
+Memory which is actively being written back to the disk
+@ mem.util.mapped Kbytes in mapped pages, from /proc/meminfo
+files which have been mmaped, such as libraries
+@ mem.util.slab Kbytes in slab memory, from /proc/meminfo
+in-kernel data structures cache
+@ mem.util.commitLimit Kbytes limit for address space commit, from /proc/meminfo
+The static total, in Kbytes, available for commitment to address 
+spaces. Thus, mem.util.committed_AS may range up to this total. Normally 
+the kernel overcommits memory, so this value may exceed mem.physmem
+@ mem.util.committed_AS Kbytes committed to address spaces, from /proc/meminfo
+An estimate of how much RAM you would need to make a 99.99% guarantee
+that there never is OOM (out of memory) for this workload. Normally
+the kernel will overcommit memory. That means, say you do a 1GB malloc,
+nothing happens, really. Only when you start USING that malloc memory
+you will get real memory on demand, and just as much as you use.
+@ mem.util.pageTables Kbytes in kernel page tables, from /proc/meminfo
+@ mem.util.reverseMaps Kbytes in reverse mapped pages, from /proc/meminfo
+@ mem.util.cache_clean Kbytes cached and not dirty or writeback, derived from /proc/meminfo
+@ mem.util.anonpages Kbytes in user pages not backed by files, from /proc/meminfo
+@ mem.util.bounce Kbytes in bounce buffers, from /proc/meminfo
+@ mem.util.NFS_Unstable Kbytes in NFS unstable memory, from /proc/meminfo
+@ mem.util.slabReclaimable Kbytes in reclaimable slab pages, from /proc/meminfo
+@ mem.util.slabUnreclaimable Kbytes in unreclaimable slab pages, from /proc/meminfo
+@ mem.util.active_anon anonymous Active list LRU memory
+@ mem.util.inactive_anon anonymous Inactive list LRU memory
+@ mem.util.active_file file-backed Active list LRU memory
+@ mem.util.inactive_file file-backed Inactive list LRU memory
+@ mem.util.unevictable kbytes of memory that is unevictable
+@ mem.util.mlocked kbytes of memory that is pinned via mlock()
+@ mem.util.shmem kbytes of shmem
+@ mem.util.kernelStack kbytes of memory used for kernel stacks
+@ mem.util.hugepagesTotal a count of total hugepages
+@ mem.util.hugepagesFree a count of free hugepages
+@ mem.util.hugepagesSurp a count of surplus hugepages
+@ mem.util.directMap4k amount of memory that is directly mapped in 4kB pages
+@ mem.util.directMap2M amount of memory that is directly mapped in 2MB pages
+@ mem.util.vmallocTotal amount of kernel memory allocated via vmalloc
+@ mem.util.vmallocUsed amount of used vmalloc memory
+@ mem.util.vmallocChunk amount of vmalloc chunk memory
+@ mem.util.mmap_copy amount of mmap_copy space (non-MMU kernels only)
+@ mem.util.quicklists amount of memory in the per-CPU quicklists
+@ mem.util.corrupthardware amount of memory in hardware corrupted pages
+@ mem.util.anonhugepages amount of memory in anonymous huge pages
+
+User memory (Kbytes) in pages not backed by files, e.g. from malloc()
+@ mem.numa.util.total per-node total memory
+@ mem.numa.util.free per-node free memory
+@ mem.numa.util.used per-node used memory
+@ mem.numa.util.active per-node Active list LRU memory
+@ mem.numa.util.inactive per-node Inactive list LRU memory
+@ mem.numa.util.active_anon per-node anonymous Active list LRU memory
+@ mem.numa.util.inactive_anon per-node anonymous Inactive list LRU memory
+@ mem.numa.util.active_file per-node file-backed Active list LRU memory
+@ mem.numa.util.inactive_file per-node file-backed Inactive list LRU memory
+@ mem.numa.util.highTotal per-node highmem total
+@ mem.numa.util.highFree per-node highmem free
+@ mem.numa.util.lowTotal per-node lowmem total
+@ mem.numa.util.lowFree per-node lowmem free
+@ mem.numa.util.unevictable per-node Unevictable memory
+@ mem.numa.util.mlocked per-node count of Mlocked memory
+@ mem.numa.util.dirty per-node dirty memory
+@ mem.numa.util.writeback per-node count of memory locked for writeback to stable storage
+@ mem.numa.util.filePages per-node count of memory backed by files
+@ mem.numa.util.mapped per-node mapped memory
+@ mem.numa.util.anonpages per-node anonymous memory
+@ mem.numa.util.shmem per-node amount of shared memory
+@ mem.numa.util.kernelStack per-node memory used as kernel stacks
+@ mem.numa.util.pageTables per-node memory used for pagetables
+@ mem.numa.util.NFS_Unstable per-node memory holding NFS data that needs writeback
+@ mem.numa.util.bounce per-node memory used for bounce buffers
+@ mem.numa.util.writebackTmp per-node temporary memory used for writeback
+@ mem.numa.util.slab per-node memory used for slab objects
+@ mem.numa.util.slabReclaimable per-node memory used for slab objects that can be reclaimed
+@ mem.numa.util.slabUnreclaimable per-node memory used for slab objects that is unreclaimable
+@ mem.numa.util.hugepagesTotal per-node total count of hugepages
+@ mem.numa.util.hugepagesFree per-node count of free hugepages
+@ mem.numa.util.hugepagesSurp per-node count of surplus hugepages
+@ mem.numa.alloc.hit per-node count of times a task wanted alloc on local node and succeeded
+@ mem.numa.alloc.miss per-node count of times a task wanted alloc on local node but got another node
+@ mem.numa.alloc.foreign count of times a task on another node alloced on that node, but got this node
+@ mem.numa.alloc.interleave_hit count of times interleaving wanted to allocate on this node and succeeded
+@ mem.numa.alloc.local_node count of times a process ran on this node and got memory on this node
+@ mem.numa.alloc.other_node count of times a process ran on this node and got memory from another node
+@ mem.vmstat.nr_dirty number of pages in dirty state
+Instantaneous number of pages in dirty state, from /proc/vmstat
+@ mem.vmstat.nr_dirtied count of pages dirtied
+Count of pages entering dirty state, from /proc/vmstat
+@ mem.vmstat.nr_writeback number of pages in writeback state
+Instantaneous number of pages in writeback state, from /proc/vmstat
+@ mem.vmstat.nr_unstable number of pages in unstable state
+Instantaneous number of pages in unstable state, from /proc/vmstat
+@ mem.vmstat.nr_page_table_pages number of page table pages
+Instantaneous number of page table pages, from /proc/vmstat
+@ mem.vmstat.nr_mapped number of mapped pagecache pages
+Instantaneous number of mapped pagecache pages, from /proc/vmstat
+See also mem.vmstat.nr_anon for anonymous mapped pages.
+@ mem.vmstat.nr_slab number of slab pages
+Instantaneous number of slab pages, from /proc/vmstat
+This counter was retired in 2.6.18 kernels, and is now the sum of
+mem.vmstat.nr_slab_reclaimable and mem.vmstat.nr_slab_unreclaimable.
+@ mem.vmstat.nr_written count of pages written out
+Count of pages written out, from /proc/vmstat
+@ mem.vmstat.numa_foreign count of foreign NUMA zone allocations
+@ mem.vmstat.numa_hit count of successful allocations from preferred NUMA zone
+@ mem.vmstat.numa_interleave count of interleaved NUMA allocations
+@ mem.vmstat.numa_local count of successful allocations from local NUMA zone
+@ mem.vmstat.numa_miss count of unsuccessful allocations from preferred NUMA zona
+@ mem.vmstat.numa_other count of unsuccessful allocations from local NUMA zone
+@ mem.vmstat.pgpgin page in operations
+Count of page in operations since boot, from /proc/vmstat
+@ mem.vmstat.pgpgout page out operations
+Count of page out operations since boot, from /proc/vmstat
+@ mem.vmstat.pswpin pages swapped in
+Count of pages swapped in since boot, from /proc/vmstat
+@ mem.vmstat.pswpout pages swapped out
+Count of pages swapped out since boot, from /proc/vmstat
+@ mem.vmstat.pgalloc_high high mem page allocations
+Count of high mem page allocations since boot, from /proc/vmstat
+@ mem.vmstat.pgalloc_normal normal mem page allocations
+Count of normal mem page allocations since boot, from /proc/vmstat
+@ mem.vmstat.pgalloc_dma dma mem page allocations
+Count of dma mem page allocations since boot, from /proc/vmstat
+@ mem.vmstat.pgalloc_dma32 dma32 mem page allocations
+Count of dma32 mem page allocations since boot, from /proc/vmstat
+@ mem.vmstat.pgalloc_movable movable mem page allocations
+Count of movable mem page allocations since boot, from /proc/vmstat
+@ mem.vmstat.pgfree page free operations
+Count of page free operations since boot, from /proc/vmstat
+@ mem.vmstat.pgactivate pages moved from inactive to active
+Count of pages moved from inactive to active since boot, from /proc/vmstat
+@ mem.vmstat.pgdeactivate pages moved from active to inactive
+Count of pages moved from active to inactive since boot, from /proc/vmstat
+@ mem.vmstat.pgfault page major and minor fault operations
+Count of page major and minor fault operations since boot, from /proc/vmstat
+@ mem.vmstat.pgmajfault major page fault operations
+Count of major page fault operations since boot, from /proc/vmstat
+@ mem.vmstat.pgrefill_high high mem pages inspected in refill_inactive_zone
+Count of high mem pages inspected in refill_inactive_zone since boot,
+from /proc/vmstat
+@ mem.vmstat.pgrefill_normal normal mem pages inspected in refill_inactive_zone
+Count of normal mem pages inspected in refill_inactive_zone since boot,
+from /proc/vmstat
+@ mem.vmstat.pgrefill_dma dma mem pages inspected in refill_inactive_zone
+Count of dma mem pages inspected in refill_inactive_zone since boot,
+from /proc/vmstat
+@ mem.vmstat.pgrefill_dma32 dma32 mem pages inspected in refill_inactive_zone
+Count of dma32 mem pages inspected in refill_inactive_zone since boot,
+from /proc/vmstat
+@ mem.vmstat.pgrefill_movable movable mem pages inspected in refill_inactive_zone
+Count of movable mem pages inspected in refill_inactive_zone since boot,
+from /proc/vmstat
+@ mem.vmstat.pgsteal_high high mem pages reclaimed
+Count of high mem pages reclaimed since boot, from /proc/vmstat
+@ mem.vmstat.pgsteal_normal normal mem pages reclaimed
+Count of normal mem pages reclaimed since boot, from /proc/vmstat
+@ mem.vmstat.pgsteal_dma dma mem pages reclaimed
+Count of dma mem pages reclaimed since boot, from /proc/vmstat
+@ mem.vmstat.pgsteal_dma32 dma32 mem pages reclaimed
+Count of dma32 mem pages reclaimed since boot, from /proc/vmstat
+@ mem.vmstat.pgsteal_movable movable mem pages reclaimed
+Count of movable mem pages reclaimed since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_kswapd_high high mem pages scanned by kswapd
+Count of high mem pages scanned by kswapd since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_kswapd_normal normal mem pages scanned by kswapd
+Count of normal mem pages scanned by kswapd since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_kswapd_dma dma mem pages scanned by kswapd
+Count of dma mem pages scanned by kswapd since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_kswapd_dma32 dma32 mem pages scanned by kswapd
+Count of dma32 mem pages scanned by kswapd since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_kswapd_movable movable mem pages scanned by kswapd
+Count of movable mem pages scanned by kswapd since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_direct_high high mem pages scanned
+Count of high mem pages scanned since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_direct_normal normal mem pages scanned
+Count of normal mem pages scanned since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_direct_dma dma mem pages scanned
+Count of dma mem pages scanned since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_direct_dma32 dma32 mem pages scanned
+Count of dma32 mem pages scanned since boot, from /proc/vmstat
+@ mem.vmstat.pgscan_direct_movable 
+Count of movable mem pages scanned since boot, from /proc/vmstat
+@ mem.vmstat.pginodesteal pages reclaimed via inode freeing
+Count of pages reclaimed via inode freeing since boot, from /proc/vmstat
+@ mem.vmstat.slabs_scanned slab pages scanned
+Count of slab pages scanned since boot, from /proc/vmstat
+@ mem.vmstat.kswapd_steal pages reclaimed by kswapd
+Count of pages reclaimed by kswapd since boot, from /proc/vmstat
+@ mem.vmstat.kswapd_low_wmark_hit_quickly count of times low watermark reached quickly
+Count of times kswapd reached low watermark quickly, from /proc/vmstat
+@ mem.vmstat.kswapd_high_wmark_hit_quickly count of times high watermark reached quickly
+Count of times kswapd reached high watermark quickly, from /proc/vmstat
+@ mem.vmstat.kswapd_skip_congestion_wait count of times kswapd skipped waiting on device congestion
+Count of times kswapd skipped waiting due to device congestion as a
+result of being under the low watermark, from /proc/vmstat
+@ mem.vmstat.kswapd_inodesteal pages reclaimed via kswapd inode freeing
+Count of pages reclaimed via kswapd inode freeing since boot, from
+/proc/vmstat
+@ mem.vmstat.pageoutrun kswapd calls to page reclaim
+Count of kswapd calls to page reclaim since boot, from /proc/vmstat
+@ mem.vmstat.allocstall direct reclaim calls
+Count of direct reclaim calls since boot, from /proc/vmstat
+@ mem.vmstat.pgrotated pages rotated to tail of the LRU
+Count of pages rotated to tail of the LRU since boot, from /proc/vmstat
+@mem.vmstat.nr_anon_pages number of anonymous mapped pagecache pages
+Instantaneous number of anonymous mapped pagecache pages, from /proc/vmstat
+See also mem.vmstat.mapped for other mapped pages.
+@mem.vmstat.nr_anon_transparent_hugepages number of anonymous transparent huge pages
+Instantaneous number of anonymous transparent huge pages, from /proc/vmstat
+@mem.vmstat.nr_bounce number of bounce buffer pages
+Instantaneous number of bounce buffer pages, from /proc/vmstat
+@mem.vmstat.nr_slab_reclaimable reclaimable slab pages
+Instantaneous number of reclaimable slab pages, from /proc/vmstat.
+@mem.vmstat.nr_slab_unreclaimable unreclaimable slab pages
+Instantaneous number of unreclaimable slab pages, from /proc/vmstat.
+@mem.vmstat.nr_vmscan_write pages written by VM scanner from LRU
+Count of pages written from the LRU by the VM scanner, from /proc/vmstat.
+The VM is supposed to minimise the number of pages which get written
+from the LRU (for IO scheduling efficiency, and for high reclaim-success
+rates).
+@ mem.vmstat.htlb_buddy_alloc_fail huge TLB page buddy allocation failures
+Count of huge TLB page buddy allocation failures, from /proc/vmstat
+@ mem.vmstat.htlb_buddy_alloc_success huge TLB page buddy allocation successes
+Count of huge TLB page buddy allocation successes, from /proc/vmstat
+@ mem.vmstat.nr_active_anon number of active anonymous memory pages
+@ mem.vmstat.nr_active_file number of active file memory memory pages
+@ mem.vmstat.nr_free_pages number of free pages
+@ mem.vmstat.nr_inactive_anon number of inactive anonymous memory pages
+@ mem.vmstat.nr_inactive_file number of inactive file memory pages
+@ mem.vmstat.nr_isolated_anon number of isolated anonymous memory pages
+@ mem.vmstat.nr_isolated_file number of isolated file memory pages
+@ mem.vmstat.nr_kernel_stack number of pages of kernel stack
+@ mem.vmstat.nr_mlock number of pages under mlock
+@ mem.vmstat.nr_shmem number of shared memory pages
+@ mem.vmstat.nr_unevictable number of unevictable pages
+@ mem.vmstat.nr_writeback_temp number of temporary writeback pages
+@ mem.vmstat.compact_blocks_moved count of compact blocks moved
+@ mem.vmstat.compact_fail count of unsuccessful compactions for high order allocations
+@ mem.vmstat.compact_pagemigrate_failed count of pages unsuccessfully compacted
+@ mem.vmstat.compact_pages_moved count of pages successfully moved for compaction
+@ mem.vmstat.compact_stall count of failures to even start compacting
+@ mem.vmstat.compact_success count of successful compactions for high order allocations
+@ mem.vmstat.thp_fault_alloc transparent huge page fault allocations
+@ mem.vmstat.thp_fault_fallback transparent huge page fault fallbacks
+@ mem.vmstat.thp_collapse_alloc transparent huge page collapse allocations
+@ mem.vmstat.thp_collapse_alloc_failed transparent huge page collapse failures
+@ mem.vmstat.thp_split count of transparent huge page splits
+@ mem.vmstat.unevictable_pgs_cleared count of unevictable pages cleared
+@ mem.vmstat.unevictable_pgs_culled count of unevictable pages culled
+@ mem.vmstat.unevictable_pgs_mlocked count of mlocked unevictable pages
+@ mem.vmstat.unevictable_pgs_mlockfreed count of unevictable pages mlock freed
+@ mem.vmstat.unevictable_pgs_munlocked count of unevictable pages munlocked
+@ mem.vmstat.unevictable_pgs_rescued count of unevictable pages rescued
+@ mem.vmstat.unevictable_pgs_scanned count of unevictable pages scanned
+@ mem.vmstat.unevictable_pgs_stranded count of unevictable pages stranded
+@ mem.vmstat.zone_reclaim_failed number of zone reclaim failures
+
+
+@ swap.length total swap available metric from /proc/meminfo
+@ swap.used swap used metric from /proc/meminfo
+@ swap.free swap free metric from /proc/meminfo
+@ kernel.all.load 1, 5 and 15 minute load average
+@ kernel.all.cpu.user total user CPU time from /proc/stat for all CPUs
+@ kernel.all.cpu.intr total interrupt CPU time from /proc/stat for all CPUs
+Total time spent processing interrupts on all CPUs.
+This value includes both soft and hard interrupt processing time.
+@ kernel.all.cpu.wait.total total wait CPU time from /proc/stat for all CPUs
+@ kernel.all.cpu.nice total nice user CPU time from /proc/stat for all CPUs
+@ kernel.all.cpu.sys total sys CPU time from /proc/stat for all CPUs
+@ kernel.all.cpu.idle total idle CPU time from /proc/stat for all CPUs
+@ kernel.all.cpu.irq.soft soft interrupt CPU time from /proc/stat for all CPUs
+Total soft interrupt CPU time (defered interrupt handling code,
+not run in the initial interrupt handler).
+@ kernel.all.cpu.irq.hard hard interrupt CPU time from /proc/stat for all CPUs
+Total soft interrupt CPU time ("hard" interrupt handling code
+is the code run directly on receipt of the initial hardware
+interrupt, and does not include "soft" interrupt handling code
+which is defered until later).
+@ kernel.all.cpu.steal total virtualisation CPU steal time for all CPUs
+Total CPU time when a CPU had a runnable process, but the hypervisor
+(virtualisation layer) chose to run something else instead.
+@ kernel.all.cpu.guest total virtual guest CPU time for all CPUs
+Total CPU time spent running virtual guest operating systems.
+@ kernel.all.nusers number of user sessions on system
+
+@ network.interface.in.bytes network recv read bytes from /proc/net/dev per network interface
+@ network.interface.in.packets network recv read packets from /proc/net/dev per network interface
+@ network.interface.in.errors network recv read errors from /proc/net/dev per network interface
+@ network.interface.in.drops network recv read drops from /proc/net/dev per network interface
+@ network.interface.in.mcasts network recv compressed from /proc/net/dev per network interface
+@ network.interface.in.fifo network recv read fifos from /proc/net/dev per network interface
+@ network.interface.in.frame network recv read frames from /proc/net/dev per network interface
+@ network.interface.in.compressed network recv compressed from /proc/net/dev per network interface
+@ network.interface.out.bytes network send bytes from /proc/net/dev per network interface
+@ network.interface.out.packets network send packets from /proc/net/dev per network interface
+@ network.interface.out.errors network send errors from /proc/net/dev per network interface
+@ network.interface.out.drops network send drops from /proc/net/dev per network interface
+@ network.interface.out.fifo network send fifos from /proc/net/dev per network interface
+@ network.interface.collisions network send collisions from /proc/net/dev per network interface
+@ network.interface.out.carrier network send carrier from /proc/net/dev per network interface
+@ network.interface.out.compressed network send compressed from /proc/net/dev per network interface
+@ network.interface.total.bytes network total (in+out) bytes from /proc/net/dev per network interface
+@ network.interface.total.packets network total (in+out) packets from /proc/net/dev per network interface
+@ network.interface.total.errors network total (in+out) errors from /proc/net/dev per network interface
+@ network.interface.total.drops network total (in+out) drops from /proc/net/dev per network interface
+@ network.interface.total.mcasts network total (in+out) mcasts from /proc/net/dev per network interface
+@ network.interface.mtu maximum transmission unit on network interface
+@ network.interface.speed interface speed in megabytes per second
+The linespeed on the network interface, as reported by the kernel,
+scaled from Megabits/second to Megabytes/second.
+See also network.interface.baudrate for the bytes/second value.
+@ network.interface.baudrate interface speed in bytes per second
+The linespeed on the network interface, as reported by the kernel,
+scaled up from Megabits/second to bits/second and divided by 8 to convert
+to bytes/second.
+See also network.interface.speed for the Megabytes/second value.
+@ network.interface.duplex value one for half or two for full duplex interface
+@ network.interface.up boolean for whether interface is currently up or down
+@ network.interface.running boolean for whether interface has resources allocated
+@ network.interface.inet_addr string INET interface address (ifconfig style)
+@ network.sockstat.tcp.inuse instantaneous number of tcp sockets currently in use
+@ network.sockstat.tcp.highest highest number of tcp sockets in use at any one time since boot
+@ network.sockstat.tcp.util instantaneous tcp socket utilization (100 * inuse/highest)
+@ network.sockstat.udp.inuse instantaneous number of udp sockets currently in use
+@ network.sockstat.udp.highest highest number of udp sockets in use at any one time since boot
+@ network.sockstat.udp.util instantaneous udp socket utilization (100 * inuse/highest)
+@ network.sockstat.raw.inuse instantaneous number of raw sockets currently in use
+@ network.sockstat.raw.highest highest number of raw sockets in use at any one time since boot
+@ network.sockstat.raw.util instantaneous raw socket utilization (100 * inuse/highest)
+@ hinv.physmem total system memory metric from /proc/meminfo
+@ hinv.pagesize Memory page size
+The memory page size of the running kernel in bytes.
+@ hinv.ncpu number of CPUs in the system
+@ hinv.ndisk number of disks in the system
+@ hinv.nfilesys number of (local) file systems currently mounted
+@ hinv.map.scsi list of active SCSI devices
+There is one string value for each SCSI device active in the system,
+as extracted from /proc/scsi/scsi. The external instance name
+for each device is in the format scsiD:C:I:L where
+D is controller number, C is channel number, I is device ID
+and L is the SCSI LUN number for the device. The values for this
+metric are the actual device names (sd[a-z] are SCSI disks, st[0-9]
+are SCSI tapes and scd[0-9] are SCSI CD-ROMS.
+@ filesys.capacity Total capacity of mounted filesystem (Kbytes)
+@ filesys.used Total space used on mounted filesystem (Kbytes)
+@ filesys.free Total space free on mounted filesystem (Kbytes)
+@ filesys.maxfiles Inodes capacity of mounted filesystem
+@ filesys.usedfiles Number of inodes allocated on mounted filesystem
+@ filesys.freefiles Number of unallocated inodes on mounted filesystem
+@ filesys.mountdir File system mount point
+@ filesys.full Percentage of filesystem in use
+@ filesys.blocksize Size of each block on mounted filesystem (Bytes)
+@ filesys.avail Total space free to non-superusers on mounted filesystem (Kbytes)
+@ filesys.readonly Indicates whether a filesystem is mounted readonly
+@ tmpfs.capacity Total capacity of mounted tmpfs filesystem (Kbytes)
+@ tmpfs.used Total space used on mounted tmpfs filesystem (Kbytes)
+@ tmpfs.free Total space free on mounted tmpfs filesystem (Kbytes)
+@ tmpfs.maxfiles Inodes capacity of mounted tmpfs filesystem
+@ tmpfs.usedfiles Number of inodes allocated on mounted tmpfs filesystem
+@ tmpfs.freefiles Number of unallocated inodes on mounted tmpfs filesystem
+@ tmpfs.full Percentage of tmpfs filesystem in use
+@ swapdev.free physical swap free space
+@ swapdev.length physical swap size
+@ swapdev.maxswap maximum swap length (same as swapdev.length on Linux)
+@ swapdev.vlength virtual swap size (always zero on Linux)
+Virtual swap size (always zero on Linux since Linux does not support
+virtual swap).
+
+This metric is retained on Linux for interoperability with PCP monitor
+tools running on IRIX.
+
+@ swapdev.priority swap resource priority
+@ nfs.client.calls cumulative total of client NFSv2 requests
+@ nfs.client.reqs cumulative total of client NFSv2 requests by request type
+@ nfs.server.calls cumulative total of server NFSv2 requests
+@ nfs.server.reqs cumulative total of client NFSv2 requests by request type
+@ nfs3.client.calls cumulative total of client NFSv3 requests
+@ nfs3.client.reqs cumulative total of client NFSv3 requests by request type
+@ nfs3.server.calls cumulative total of server NFSv3 requests
+@ nfs3.server.reqs cumulative total of client NFSv3 requests by request type
+@ nfs4.client.calls cumulative total of client NFSv4 requests
+@ nfs4.client.reqs cumulative total for each client NFSv4 request type
+@ nfs4.server.calls cumulative total of server NFSv4 operations, plus NULL requests
+@ nfs4.server.reqs cumulative total for each server NFSv4 operation, and for NULL requests
+@ rpc.client.rpccnt cumulative total of client RPC requests
+@ rpc.client.rpcretrans cumulative total of client RPC retransmissions
+@ rpc.client.rpcauthrefresh cumulative total of client RPC auth refreshes
+@ rpc.client.netcnt cumulative total of client RPC network layer requests
+@ rpc.client.netudpcnt cumulative total of client RPC UDP network layer requests
+@ rpc.client.nettcpcnt cumulative total of client RPC TCP network layer requests
+@ rpc.client.nettcpconn cumulative total of client RPC TCP network layer connection requests
+@ rpc.server.rpccnt cumulative total of server RPC requests
+@ rpc.server.rpcerr cumulative total of server RPC errors
+@ rpc.server.rpcbadfmt cumulative total of server RPC bad format errors
+@ rpc.server.rpcbadauth cumulative total of server RPC bad auth errors
+@ rpc.server.rpcbadclnt cumulative total of server RPC bad client errors
+@ rpc.server.rchits cumulative total of request-reply-cache hits
+@ rpc.server.rcmisses cumulative total of request-reply-cache misses
+@ rpc.server.rcnocache cumulative total of uncached request-reply-cache requests
+@ rpc.server.fh_cached cumulative total of file handle cache requests
+@ rpc.server.fh_valid cumulative total of file handle cache validations
+@ rpc.server.fh_fixup cumulative total of file handle cache fixup validations
+@ rpc.server.fh_lookup cumulative total of file handle cache new lookups
+@ rpc.server.fh_stale cumulative total of stale file handle cache errors
+@ rpc.server.fh_concurrent cumulative total of concurrent file handle cache requests
+@ rpc.server.netcnt cumulative total of server RPC network layer requests
+@ rpc.server.netudpcnt cumulative total of server RPC UDP network layer requests
+@ rpc.server.nettcpcnt cumulative total of server RPC TCP network layer requests
+@ rpc.server.nettcpconn cumulative total of server RPC TCP network layer connection requests
+@ rpc.server.fh_anon cumulative total anonymous file dentries returned
+@ rpc.server.fh_nocache_dir count of directory file handles not found cached
+@ rpc.server.fh_nocache_nondir count of non-directory file handles not found cached
+@ rpc.server.io_read cumulative count of bytes returned from read requests
+@ rpc.server.io_write cumulative count of bytes passed into write requests
+@ rpc.server.th_cnt available nfsd threads
+@ rpc.server.th_fullcnt number of times the last free nfsd thread was used
+
+@ network.ip.forwarding count of ip forwarding
+@ network.ip.defaultttl count of ip defaultttl
+@ network.ip.inreceives count of ip inreceives
+@ network.ip.inhdrerrors count of ip inhdrerrors
+@ network.ip.inaddrerrors count of ip inaddrerrors
+@ network.ip.forwdatagrams count of ip forwdatagrams
+@ network.ip.inunknownprotos count of ip inunknownprotos
+@ network.ip.indiscards count of ip indiscards
+@ network.ip.indelivers count of ip indelivers
+@ network.ip.outrequests count of ip outrequests
+@ network.ip.outdiscards count of ip outdiscards
+@ network.ip.outnoroutes count of ip outnoroutes
+@ network.ip.reasmtimeout count of ip reasmtimeout
+@ network.ip.reasmreqds count of ip reasmreqds
+@ network.ip.reasmoks count of ip reasmoks
+@ network.ip.reasmfails count of ip reasmfails
+@ network.ip.fragoks count of ip fragoks
+@ network.ip.fragfails count of ip fragfails
+@ network.ip.fragcreates count of ip fragcreates
+@ network.icmp.inmsgs count of icmp inmsgs
+@ network.icmp.inerrors count of icmp inerrors
+@ network.icmp.indestunreachs count of icmp indestunreachs
+@ network.icmp.intimeexcds count of icmp intimeexcds
+@ network.icmp.inparmprobs count of icmp inparmprobs
+@ network.icmp.insrcquenchs count of icmp insrcquenchs
+@ network.icmp.inredirects count of icmp inredirects
+@ network.icmp.inechos count of icmp inechos
+@ network.icmp.inechoreps count of icmp inechoreps
+@ network.icmp.intimestamps count of icmp intimestamps
+@ network.icmp.intimestampreps count of icmp intimestampreps
+@ network.icmp.inaddrmasks count of icmp inaddrmasks
+@ network.icmp.inaddrmaskreps count of icmp inaddrmaskreps
+@ network.icmp.outmsgs count of icmp outmsgs
+@ network.icmp.outerrors count of icmp outerrors
+@ network.icmp.outdestunreachs count of icmp outdestunreachs
+@ network.icmp.outtimeexcds count of icmp outtimeexcds
+@ network.icmp.outparmprobs count of icmp outparmprobs
+@ network.icmp.outsrcquenchs count of icmp outsrcquenchs
+@ network.icmp.outredirects count of icmp outredirects
+@ network.icmp.outechos count of icmp outechos
+@ network.icmp.outechoreps count of icmp outechoreps
+@ network.icmp.outtimestamps count of icmp outtimestamps
+@ network.icmp.outtimestampreps count of icmp outtimestampreps
+@ network.icmp.outaddrmasks count of icmp outaddrmasks
+@ network.icmp.outaddrmaskreps count of icmp outaddrmaskreps
+@ network.tcp.rtoalgorithm count of tcp rtoalgorithm
+@ network.tcp.rtomin count of tcp rtomin
+@ network.tcp.rtomax count of tcp rtomax
+@ network.tcp.maxconn count of tcp maxconn
+@ network.tcp.activeopens count of tcp activeopens
+@ network.tcp.passiveopens count of tcp passiveopens
+@ network.tcp.attemptfails count of tcp attemptfails
+@ network.tcp.estabresets count of tcp estabresets
+@ network.tcp.currestab count of tcp currestab
+@ network.tcp.insegs count of tcp insegs
+@ network.tcp.outsegs count of tcp outsegs
+@ network.tcp.retranssegs count of tcp retranssegs
+@ network.tcp.inerrs count of tcp inerrs
+@ network.tcp.outrsts count of tcp outrsts
+@ network.tcpconn.established Number of established connections
+@ network.tcpconn.syn_sent Number of SYN_SENT connections
+@ network.tcpconn.syn_recv Number of SYN_RECV connections
+@ network.tcpconn.fin_wait1 Number of FIN_WAIT1 connections
+@ network.tcpconn.fin_wait2 Number of FIN_WAIT2 connections
+@ network.tcpconn.time_wait Number of TIME_WAIT connections
+@ network.tcpconn.close Number of CLOSE connections
+@ network.tcpconn.close_wait Number of CLOSE_WAIT connections
+@ network.tcpconn.last_ack Number of LAST_ACK connections
+@ network.tcpconn.listen Number of LISTEN connections
+@ network.tcpconn.closing Number of CLOSING connections
+@ network.udp.indatagrams count of udp indatagrams
+@ network.udp.noports count of udp noports
+@ network.udp.inerrors count of udp inerrors
+@ network.udp.outdatagrams count of udp outdatagrams
+@ network.udp.recvbuferrors count of udp receive buffer errors
+@ network.udp.sndbuferrors count of udp send buffer errors
+@ network.udplite.indatagrams count of udplite indatagrams
+@ network.udplite.noports count of udplite noports
+@ network.udplite.inerrors count of udplite inerrors
+@ network.udplite.outdatagrams count of udplite outdatagrams
+@ network.udplite.recvbuferrors count of udplite receive buffer errors
+@ network.udplite.sndbuferrors count of udplite send buffer errors
+@ xfs.allocs.alloc_extent XFS extents allocated
+Number of file system extents allocated over all XFS filesystems
+@ xfs.allocs.alloc_block XFS blocks allocated
+Number of file system blocks allocated over all XFS filesystems
+@ xfs.allocs.free_extent XFS extents freed
+Number of file system extents freed over all XFS filesystems
+@ xfs.allocs.free_block XFS blocks freed
+Number of file system blocks freed over all XFS filesystems
+
+@ xfs.alloc_btree.lookup lookups in XFS alloc btrees
+Number of lookup operations in XFS filesystem allocation btrees
+@ xfs.alloc_btree.compare compares in XFS alloc btrees
+Number of compares in XFS filesystem allocation btree lookups
+@ xfs.alloc_btree.insrec insertions in XFS alloc btrees
+Number of extent records inserted into XFS filesystem allocation btrees
+@ xfs.alloc_btree.delrec deletions in XFS alloc btrees
+Number of extent records deleted from XFS filesystem allocation btrees
+
+@ xfs.block_map.read_ops block map read ops in XFS
+Number of block map for read operations performed on XFS files
+@ xfs.block_map.write_ops block map write ops in XFS
+Number of block map for write operations performed on XFS files
+@ xfs.block_map.unmap block unmap ops in XFS
+Number of block unmap (delete) operations performed on XFS files
+@ xfs.block_map.add_exlist extent list add ops in XFS
+Number of extent list insertion operations for XFS files
+@ xfs.block_map.del_exlist extent list delete ops in XFS
+Number of extent list deletion operations for XFS files
+@ xfs.block_map.look_exlist extent list lookup ops in XFS
+Number of extent list lookup operations for XFS files
+@ xfs.block_map.cmp_exlist extent list compare ops in XFS
+Number of extent list comparisons in XFS extent list lookups
+
+@ xfs.bmap_btree.lookup block map btree lookup ops in XFS
+Number of block map btree lookup operations on XFS files
+@ xfs.bmap_btree.compare block map btree compare ops in XFS
+Number of block map btree compare operations in XFS block map lookups
+@ xfs.bmap_btree.insrec block map btree insert ops in XFS
+Number of block map btree records inserted for XFS files
+@ xfs.bmap_btree.delrec block map btree delete ops in XFS
+Number of block map btree records deleted for XFS files
+
+@ xfs.dir_ops.lookup number of file name directory lookups
+This is a count of the number of file name directory lookups in XFS
+filesystems. It counts only those lookups which miss in the operating
+system's directory name lookup cache and must search the real directory
+structure for the name in question.  The count is incremented once for
+each level of a pathname search that results in a directory lookup.
+
+@ xfs.dir_ops.create number of directory entry creation operations
+This is the number of times a new directory entry was created in XFS
+filesystems. Each time that a new file, directory, link, symbolic link,
+or special file is created in the directory hierarchy the count is
+incremented.
+
+@ xfs.dir_ops.remove number of directory entry remove operations
+This is the number of times an existing directory entry was removed in
+XFS filesystems. Each time that a file, directory, link, symbolic link,
+or special file is removed from the directory hierarchy the count is
+incremented.
+
+@ xfs.dir_ops.getdents number of times the getdents operation is performed
+This is the number of times the XFS directory getdents operation was
+performed. The getdents operation is used by programs to read the
+contents of directories in a file system independent fashion.  This
+count corresponds exactly to the number of times the getdents(2) system
+call was successfully used on an XFS directory.
+
+@ xfs.transactions.sync number of synchronous meta-data transactions performed
+This is the number of meta-data transactions which waited to be
+committed to the on-disk log before allowing the process performing the
+transaction to continue. These transactions are slower and more
+expensive than asynchronous transactions, because they force the in
+memory log buffers to be forced to disk more often and they wait for
+the completion of the log buffer writes. Synchronous transactions
+include file truncations and all directory updates when the file system
+is mounted with the 'wsync' option.
+
+@ xfs.transactions.async number of asynchronous meta-data transactions performed
+This is the number of meta-data transactions which did not wait to be
+committed to the on-disk log before allowing the process performing the
+transaction to continue. These transactions are faster and more
+efficient than synchronous transactions, because they commit their data
+to the in memory log buffers without forcing those buffers to be
+written to disk. This allows multiple asynchronous transactions to be
+committed to disk in a single log buffer write. Most transactions used
+in XFS file systems are asynchronous.
+
+@ xfs.transactions.empty number of meta-data transactions which committed without changing anything
+This is the number of meta-data transactions which did not actually
+change anything. These are transactions which were started for some
+purpose, but in the end it turned out that no change was necessary.
+
+@ xfs.inode_ops.ig_attempts number of in memory inode lookup operations
+This is the number of times the operating system looked for an XFS
+inode in the inode cache. Whether the inode was found in the cache or
+needed to be read in from the disk is not indicated here, but this can
+be computed from the ig_found and ig_missed counts.
+
+@ xfs.inode_ops.ig_found number of successful in memory inode lookup operations
+This is the number of times the operating system looked for an XFS
+inode in the inode cache and found it. The closer this count is to the
+ig_attempts count the better the inode cache is performing.
+
+@ xfs.inode_ops.ig_frecycle number of just missed in memory inode lookup operations
+This is the number of times the operating system looked for an XFS
+inode in the inode cache and saw that it was there but was unable to
+use the in memory inode because it was being recycled by another
+process.
+
+@ xfs.inode_ops.ig_missed number of failed in memory inode lookup operations
+This is the number of times the operating system looked for an XFS
+inode in the inode cache and the inode was not there. The further this
+count is from the ig_attempts count the better.
+
+@ xfs.inode_ops.ig_dup number of inode cache insertions that fail because the inode is there
+This is the number of times the operating system looked for an XFS
+inode in the inode cache and found that it was not there but upon
+attempting to add the inode to the cache found that another process had
+already inserted it.
+
+@ xfs.inode_ops.ig_reclaims number of in memory inode recycle operations
+This is the number of times the operating system recycled an XFS inode
+from the inode cache in order to use the memory for that inode for
+another purpose. Inodes are recycled in order to keep the inode cache
+from growing without bound. If the reclaim rate is high it may be
+beneficial to raise the vnode_free_ratio kernel tunable variable to
+increase the size of inode cache.
+
+@ xfs.inode_ops.ig_attrchg number of inode attribute change operations
+This is the number of times the operating system explicitly changed the
+attributes of an XFS inode. For example, this could be to change the
+inode's owner, the inode's size, or the inode's timestamps.
+
+@ xfs.log.writes number of buffer writes going to the disk from the log
+This variable counts the number of log buffer writes going to the
+physical log partitions of all XFS filesystems. Log data traffic is
+proportional to the level of meta-data updating. Log buffer writes get
+generated when they fill up or external syncs occur.
+
+@ xfs.log.blocks write throughput to the physical XFS log
+This variable counts the number of Kbytes of information being written
+to the physical log partitions of all XFS filesystems. Log data traffic
+is proportional to the level of meta-data updating. The rate with which
+log data gets written depends on the size of internal log buffers and
+disk write speed. Therefore, filesystems with very high meta-data
+updating may need to stripe the log partition or put the log partition
+on a separate drive.
+
+@ xfs.log.write_ratio ratio of count of XFS log blocks written to log writes
+The ratio of log blocks written to log writes.  If block count isn't a
+"reasonable" multiple of writes, then many small log writes are being
+performed - this is suboptimal.  Perfection is 64.  Fine-grain control
+can be obtained when this metric is used in conjuntion with pmstore(1)
+and the xfs.control.reset metric.
+
+@ xfs.log.noiclogs count of failures for immediate get of buffered/internal
+This variable keeps track of times when a logged transaction can not
+get any log buffer space. When this occurs, all of the internal log
+buffers are busy flushing their data to the physical on-disk log.
+
+@ xfs.log.force value from xs_log_force field of struct xfsstats
+The number of times the in-core log is forced to disk.  It is
+equivalent to the number of successful calls to the function
+xfs_log_force().
+
+@ xfs.log.force_sleep value from xs_log_force_sleep field of struct xfsstats
+This metric is exported from the xs_log_force_sleep field of struct xfsstats
+
+@ xfs.log_tail.try_logspace value from xs_try_logspace field of struct xfsstats
+This metric is exported from the xs_try_logspace field of struct xfsstats
+
+@ xfs.log_tail.sleep_logspace value from xs_sleep_logspace field of struct xfsstats
+This metric is exported from the xs_sleep_logspace field of struct xfsstats
+
+@ xfs.log_tail.push_ail.pushes number of times the AIL tail is moved forward
+The number of times the tail of the AIL is moved forward.  It is
+equivalent to the number of successful calls to the function
+xfs_trans_push_ail().
+
+@ xfs.log_tail.push_ail.success value from xs_push_ail_success field of struct xfsstats
+@ xfs.log_tail.push_ail.pushbuf value from xs_push_ail_pushbuf field of struct xfsstats
+@ xfs.log_tail.push_ail.pinned value from xs_push_ail_pinned field of struct xfsstats
+@ xfs.log_tail.push_ail.locked value from xs_push_ail_locked field of struct xfsstats
+@ xfs.log_tail.push_ail.flushing value from xs_push_ail_flushing field of struct xfsstats
+@ xfs.log_tail.push_ail.restarts value from xs_push_ail_restarts field of struct xfsstats
+@ xfs.log_tail.push_ail.flush value from xs_push_ail_flush field of struct xfsstats
+
+@ xfs.xstrat.bytes number of bytes of data processed by the XFS daemons
+This is the number of bytes of file data flushed out by the XFS
+flushing daemons.
+
+@ xfs.xstrat.quick number of buffers processed by the XFS daemons written to contiguous space on disk
+This is the number of buffers flushed out by the XFS flushing daemons
+which are written to contiguous space on disk. The buffers handled by
+the XFS daemons are delayed allocation buffers, so this count gives an
+indication of the success of the XFS daemons in allocating contiguous
+disk space for the data being flushed to disk.
+
+@ xfs.xstrat.split number of buffers processed by the XFS daemons written to non-contiguous space on disk
+This is the number of buffers flushed out by the XFS flushing daemons
+which are written to non-contiguous space on disk. The buffers handled
+by the XFS daemons are delayed allocation buffers, so this count gives
+an indication of the failure of the XFS daemons in allocating
+contiguous disk space for the data being flushed to disk. Large values
+in this counter indicate that the file system has become fragmented.
+
+@ xfs.write number of XFS file system write operations
+This is the number of write(2) system calls made to files in
+XFS file systems.
+
+@ xfs.write_bytes number of bytes written in XFS file system write operations
+This is the number of bytes written via write(2) system calls to files
+in XFS file systems. It can be used in conjunction with the write_calls
+count to calculate the average size of the write operations to files in
+XFS file systems.
+
+@ xfs.read number of XFS file system read operations
+This is the number of read(2) system calls made to files in XFS file
+systems.
+
+@ xfs.read_bytes number of bytes read in XFS file system read operations
+This is the number of bytes read via read(2) system calls to files in
+XFS file systems. It can be used in conjunction with the read_calls
+count to calculate the average size of the read operations to files in
+XFS file systems.
+
+@ xfs.attr.get number of "get" operations on XFS extended file attributes
+The number of "get" operations performed on extended file attributes
+within XFS filesystems.  The "get" operation retrieves the value of an
+extended attribute.
+
+@ xfs.attr.set number of "set" operations on XFS extended file attributes
+The number of "set" operations performed on extended file attributes
+within XFS filesystems.  The "set" operation creates and sets the value
+of an extended attribute.
+
+@ xfs.attr.remove number of "remove" operations on XFS extended file attributes
+The number of "remove" operations performed on extended file attributes
+within XFS filesystems.  The "remove" operation deletes an extended
+attribute.
+
+@ xfs.attr.list number of "list" operations on XFS extended file attributes
+The number of "list" operations performed on extended file attributes
+within XFS filesystems.  The "list" operation retrieves the set of
+extended attributes associated with a file.
+
+@ xfs.quota.reclaims value from xs_qm_dqreclaims field of struct xfsstats
+@ xfs.quota.reclaim_misses value from xs_qm_dqreclaim_misses field of struct xfsstats
+@ xfs.quota.dquot_dups value from xs_qm_dquot_dups field of struct xfsstats
+@ xfs.quota.cachemisses value from xs_qm_dqcachemisses field of struct xfsstats
+@ xfs.quota.cachehits value from xs_qm_dqcachehits field of struct xfsstats
+@ xfs.quota.wants value from xs_qm_dqwants field of struct xfsstats
+@ xfs.quota.shake_reclaims value from xs_qm_dqshake_reclaims field of struct xfsstats
+@ xfs.quota.inact_reclaims value from xs_qm_dqinact_reclaims field of struct xfsstats
+
+@ xfs.iflush_count the number of calls to xfs_iflush
+This is the number of calls to xfs_iflush which gets called when an
+inode is being flushed (such as by bdflush or tail pushing).
+xfs_iflush searches for other inodes in the same cluster which are
+dirty and flushable.
+
+@ xfs.icluster_flushcnt value from xs_icluster_flushcnt field of struct xfsstats
+
+@ xfs.icluster_flushinode number of flushes of only one inode in cluster
+This is the number of times that the inode clustering was not able to
+flush anything but the one inode it was called with.
+
+@ xfs.buffer.get number of request buffer calls
+@ xfs.buffer.create number of buffers created
+@ xfs.buffer.get_locked number of requests for a locked buffer which succeeded
+@ xfs.buffer.get_locked_waited number of requests for a locked buffer which waited
+@ xfs.buffer.miss_locked number of requests for a locked buffer which failed due to no buffer
+@ xfs.buffer.busy_locked number of non-blocking requests for a locked buffer which failed
+@ xfs.buffer.page_retries number of retry attempts when allocating a page for insertion in a buffer
+@ xfs.buffer.page_found number of hits in the page cache when looking for a page
+@ xfs.buffer.get_read number of buffer get calls requiring immediate device reads
+@ xfs.vnodes.active number of vnodes not on free lists
+@ xfs.vnodes.alloc number of times vn_alloc called
+@ xfs.vnodes.get number of times vn_get called
+@ xfs.vnodes.hold number of times vn_hold called
+@ xfs.vnodes.rele number of times vn_rele called
+@ xfs.vnodes.reclaim number of times vn_reclaim called
+@ xfs.vnodes.remove number of times vn_remove called
+@ xfs.vnodes.free number of times vn_free called
+@ xfs.control.reset reset the values of all XFS metrics to zero
+
+@ pmda.uname identity and type of current system
+Identity and type of current system.  The concatenation of the values
+returned from utsname(2), also similar to uname -a.
+
+See also the kernel.uname.* metrics
+
+@ pmda.version build version of Linux PMDA
+@ hinv.map.cpu_num logical to physical CPU mapping for each CPU
+@ hinv.machine machine name, IP35 if SGI SNIA, else simply linux
+@ hinv.cpu.clock clock rate in Mhz for each CPU as reported by /proc/cpuinfo
+@ hinv.cpu.vendor manafacturer of each CPU as reported by /proc/cpuinfo
+@ hinv.cpu.model model name of each CPU as reported by /proc/cpuinfo
+@ hinv.cpu.stepping stepping of each CPU as reported by /proc/cpuinfo
+@ hinv.cpu.cache primary cache size of each CPU as reported by /proc/cpuinfo
+@ hinv.cpu.bogomips bogo mips rating for each CPU as reported by /proc/cpuinfo
+@ kernel.all.hz value of HZ (jiffies/second) for the currently running kernel
+@ kernel.all.uptime time the current kernel has been running
+@ kernel.all.idletime time the current kernel has been idle since boot
+@ kernel.all.lastpid most recently allocated process id
+@ kernel.all.runnable total number of processes in the (per-CPU) run queues
+@ kernel.all.nprocs total number of processes (lightweight)
+@ mem.slabinfo.objects.active number of active objects in each cache
+@ mem.slabinfo.objects.total total number of objects in each cache
+@ mem.slabinfo.objects.size size of individual objects of each cache
+@ mem.slabinfo.slabs.active number of active slabs comprising each cache
+@ mem.slabinfo.slabs.total total number of slabs comprising each cache
+@ mem.slabinfo.slabs.pages_per_slab number of pages in each slab
+@ mem.slabinfo.slabs.objects_per_slab number of objects in each slab
+@ mem.slabinfo.slabs.total_size total number of bytes allocated for active objects in each slab
+@ ipc.sem.max_semmap  maximum number of entries in a semaphore map (from semctl(..,IPC_INFO,..))
+@ ipc.sem.max_semid maximum number of semaphore identifiers (from semctl(..,IPC_INFO,..))
+@ ipc.sem.max_sem maximum number of semaphores in system (from semctl(..,IPC_INFO,..))
+@ ipc.sem.num_undo number of undo structures in system (from semctl(..,IPC_INFO,..))
+@ ipc.sem.max_perid maximum number of semaphores per identifier (from semctl(..,IPC_INFO,..))
+@ ipc.sem.max_ops maximum number of operations per semop call (from semctl(..,IPC_INFO,..))
+@ ipc.sem.max_undoent maximum number of undo entries per process (from semctl(..,IPC_INFO,..))
+@ ipc.sem.sz_semundo size of struct sem_undo (from semctl(..,IPC_INFO,..))
+@ ipc.sem.max_semval semaphore maximum value (from semctl(..,IPC_INFO,..))
+@ ipc.sem.max_exit adjust on exit maximum value (from semctl(..,IPC_INFO,..))
+@ ipc.msg.sz_pool size of message pool in kilobytes (from msgctl(..,IPC_INFO,..))
+@ ipc.msg.mapent number of entries in a message map (from msgctl(..,IPC_INFO,..))
+@ ipc.msg.max_msgsz maximum size of a message in bytes (from msgctl(..,IPC_INFO,..))
+@ ipc.msg.max_defmsgq default maximum size of a message queue (from msgctl(..,IPC_INFO,..))
+@ ipc.msg.max_msgqid maximum number of message queue identifiers (from msgctl(..,IPC_INFO,..))
+@ ipc.msg.max_msgseg message segment size (from msgctl(..,IPC_INFO,..))
+@ ipc.msg.num_smsghdr number of system message headers (from msgctl(..,IPC_INFO,..))
+@ ipc.msg.max_seg maximum number of message segments (from msgctl(..,IPC_INFO,..))
+@ ipc.shm.max_segsz maximum shared segment size in bytes (from shmctl(..,IPC_INFO,..))
+@ ipc.shm.min_segsz minimum shared segment size in bytes (from shmctl(..,IPC_INFO,..))
+@ ipc.shm.max_seg maximum number of shared segments in system (from shmctl(..,IPC_INFO,..))
+@ ipc.shm.max_segproc maximum number of shared segments per process (from shmctl(..,IPC_INFO,..))
+@ ipc.shm.max_shmsys maximum amount of shared memory in system in pages (from shmctl(..,IPC_INFO,..))
+
+@ vfs.files.count number of in-use file structures
+@ vfs.files.free number of available file structures
+@ vfs.files.max hard maximum on number of file structures
+@ vfs.inodes.count number of in-use inode structures
+@ vfs.inodes.free number of available inode structures
+@ vfs.dentry.count number of in-use dentry structures
+@ vfs.dentry.free number of available dentry structures
+
+@ network.ib.status (deprecated: use infiniband.*) IB status and description from ibstatus
+@ network.ib.in.bytes (deprecated: use infiniband.*) network recv read bytes from perfquery
+@ network.ib.in.packets (deprecated: use infiniband.*) network recv read packets from perfquery
+@ network.ib.in.errors.drop (deprecated: use infiniband.*) network recv read drops from perfquery
+@ network.ib.in.errors.filter (deprecated: use infiniband.*) network recv filtered packets from perfquery
+@ network.ib.in.errors.local (deprecated: use infiniband.*) network recv error packets from perfquery
+@ network.ib.in.errors.remote (deprecated: use infiniband.*) remote phys error packets recv ...
+remote phys error packets recv with EBP delim from perfquery
+@ network.ib.out.bytes (deprecated: use infiniband.*) network send bytes from perfquery
+@ network.ib.out.packets (deprecated: use infiniband.*) network send packets from perfquery
+@ network.ib.out.errors.drop (deprecated: use infiniband.*) network send drops from perfquery
+@ network.ib.out.errors.filter (deprecated: use infiniband.*) network send filtered packets from perfquery
+@ network.ib.total.bytes (deprecated: use infiniband.*) network total (in+out) bytes from perfquery
+@ network.ib.total.packets (deprecated: use infiniband.*) network total (in+out) packets from perfquery
+@ network.ib.total.errors.drop (deprecated: use infiniband.*) network drops (in+out) from perfquery
+@ network.ib.total.errors.filter (deprecated: use infiniband.*) network filtered packets (in+out) from perfquery
+@ network.ib.total.errors.link (deprecated: use infiniband.*) network link drops from perfquery
+@ network.ib.total.errors.recover (deprecated: use infiniband.*) network link recoveries from perfquery
+@ network.ib.total.errors.integrity (deprecated: use infiniband.*) local link integrity errors from perfquery
+@ network.ib.total.errors.vl15 (deprecated: use infiniband.*) VL15 packets dropped from perfquery
+@ network.ib.total.errors.overrun (deprecated: use infiniband.*) excessive buffer overrun errors from perfquery
+@ network.ib.total.errors.symbol (deprecated: use infiniband.*) minor link errors from perfquery
+@ network.ib.control (deprecated: use infiniband.*) Control IB stats collection.
+If non-zero, stop collecting perf stats this many secs after last fetch
+
+@ quota.state.project.accounting 1 indicates quota accounting enabled, else 0
+@ quota.state.project.enforcement 1 indicates quotas enforced, else 0
+@ quota.project.space.hard hard limit for this project and filesys in Kbytes
+@ quota.project.space.soft soft limit for this project and filesys in Kbytes
+@ quota.project.space.used space used for this project and filesys in Kbytes
+@ quota.project.space.time_left when soft limit is exceeded, seconds until it is enacted
+@ quota.project.files.hard file count hard limit for this project and filesys
+@ quota.project.files.soft file count soft limit for this project and filesys
+@ quota.project.files.used file count for this project and filesys
+@ quota.project.files.time_left when soft limit is exceeded, seconds until it is enacted
+
+@ sysfs.kernel.uevent_seqnum counter of the number of uevents processed by the udev subsystem
+
+@ xfs.btree.alloc_blocks.lookup
+Number of free-space-by-block-number btree record lookups
+@ xfs.btree.alloc_blocks.compare
+Number of free-space-by-block-number btree record compares
+@ xfs.btree.alloc_blocks.insrec
+Number of free-space-by-block-number btree insert record operations executed
+@ xfs.btree.alloc_blocks.delrec
+Number of free-space-by-block-number btree delete record operations executed
+@ xfs.btree.alloc_blocks.newroot
+Number of times a new level is added to a free-space-by-block-number btree
+@ xfs.btree.alloc_blocks.killroot
+Number of times a level is removed from a free-space-by-block-number btree
+@ xfs.btree.alloc_blocks.increment
+Number of times a cursor has been moved forward one free-space-by-block-number
+btree record
+@ xfs.btree.alloc_blocks.decrement
+Number of times a cursor has been moved backward one free-space-by-block-number
+btree record
+@ xfs.btree.alloc_blocks.lshift
+Left shift block operations to make space for a new free-space-by-block-number
+btree record
+@ xfs.btree.alloc_blocks.rshift
+Right shift block operations to make space for a new free-space-by-block-number
+btree record
+@ xfs.btree.alloc_blocks.split
+Split block operations to make space for a new free-space-by-block-number
+btree record
+@ xfs.btree.alloc_blocks.join
+Merge block operations when deleting free-space-by-block-number btree records
+@ xfs.btree.alloc_blocks.alloc
+Btree block allocations during free-space-by-block-number btree operations
+@ xfs.btree.alloc_blocks.free
+Btree blocks freed during free-space-by-block-number btree operations
+@ xfs.btree.alloc_blocks.moves
+Records moved inside blocks during free-space-by-block-number btree operations
+
+
+@ xfs.btree.alloc_contig.lookup
+Number of free-space-by-size btree record lookups
+@ xfs.btree.alloc_contig.compare
+Number of free-space-by-size btree btree record compares
+@ xfs.btree.alloc_contig.insrec
+Number of free-space-by-size btree insert record operations executed
+@ xfs.btree.alloc_contig.delrec
+Number of free-space-by-size btree delete record operations executed
+@ xfs.btree.alloc_contig.newroot
+Number of times a new level is added to a free-space-by-size btree tree
+@ xfs.btree.alloc_contig.killroot
+Number of times a level is removed from a free-space-by-size btree tree
+@ xfs.btree.alloc_contig.increment
+Number of times a free-space-by-size btree cursor has been moved forward
+one record
+@ xfs.btree.alloc_contig.decrement
+Number of times a free-space-by-size btree cursor has been moved backward
+one record
+@ xfs.btree.alloc_contig.lshift
+Left shift block operations to make space for a new free-space-by-size
+btree record
+@ xfs.btree.alloc_contig.rshift
+Right shift block operations to make space for a new free-space-by-size
+btree record
+@ xfs.btree.alloc_contig.split
+Split block operations to make space for a new free-space-by-size btree
+record
+@ xfs.btree.alloc_contig.join
+Merge block operations when deleting free-space-by-size btree records
+@ xfs.btree.alloc_contig.alloc
+Btree block allocations during free-space-by-size btree operations
+@ xfs.btree.alloc_contig.free
+Btree blocks freed during free-space-by-size btree operations
+@ xfs.btree.alloc_contig.moves
+Records moved inside blocks during free-space-by-size btree operations
+
+@ xfs.btree.block_map.lookup
+Number of inode-block-map/extent btree record lookups
+@ xfs.btree.block_map.compare
+Number of inode-block-map/extent btree record compares
+@ xfs.btree.block_map.insrec
+Number of inode-block-map/extent btree insert record operations executed
+@ xfs.btree.block_map.delrec
+Number of inode-block-map/extent btree delete record operations executed
+@ xfs.btree.block_map.newroot
+Number of times a new level is added to an inode-block-map/extent btree
+@ xfs.btree.block_map.killroot
+Number of times a level is removed from an inode-block-map/extent btree
+@ xfs.btree.block_map.increment
+Number of times an inode-block-map/extent btree cursor has been moved
+forward one record
+@ xfs.btree.block_map.decrement
+Number of times an inode-block-map/extent btree cursor has been moved
+backward one record
+@ xfs.btree.block_map.lshift
+Left shift block operations to make space for a new inode-block-map/extent
+btree record
+@ xfs.btree.block_map.rshift
+Right shift block operations to make space for a new inode-block-map/extent
+btree record
+@ xfs.btree.block_map.split
+Split block operations to make space for a new inode-block-map/extent
+btree record
+@ xfs.btree.block_map.join
+Merge block operations when deleting inode-block-map/extent btree records
+@ xfs.btree.block_map.alloc
+Btree block allocations during inode-block-map/extent btree operations
+@ xfs.btree.block_map.free
+Btree blocks freed during inode-block-map/extent btree operations
+@ xfs.btree.block_map.moves
+Records moved inside blocks during inode-block-map/extent btree operations
+
+@ xfs.btree.inode.lookup
+Number of inode-allocation btree record lookups
+@ xfs.btree.inode.compare
+Number of inode-allocation btree record compares
+@ xfs.btree.inode.insrec
+Number of inode-allocation btree insert record operations executed
+@ xfs.btree.inode.delrec
+Number of inode-allocation btree delete record operations executed
+@ xfs.btree.inode.newroot
+Number of times a new level is added to an inode-allocation btree
+@ xfs.btree.inode.killroot
+Number of times a level is removed from an inode-allocation btree
+@ xfs.btree.inode.increment
+Number of times a cursor has been moved forward one inode-allocation
+btree record
+@ xfs.btree.inode.decrement
+Number of times a cursor has been moved backward one inode-allocation
+btree record
+@ xfs.btree.inode.lshift
+Left shift block operations to make space for a new inode-allocation
+btree record
+@ xfs.btree.inode.rshift
+Right shift block operations to make space for a new inode-allocation
+btree record
+@ xfs.btree.inode.split
+Split block operations to make space for a new inode-allocation btree record
+@ xfs.btree.inode.join
+Merge block operations when deleting inode-allocation btree records
+@ xfs.btree.inode.alloc
+Btree block allocations during inode-allocation btree operations
+@ xfs.btree.inode.free
+Btree blocks freed during inode-allocation btree operations
+@ xfs.btree.inode.moves
+Records moved inside blocks during inode-allocation btree operations
+
+
--- pcp-pmda-cluster-1.0.1/src/cluster.h.orig	2009-10-20 15:27:18.000000000 +0200
+++ pcp-pmda-cluster-1.0.1/src/cluster.h	2012-12-26 21:16:19.385766557 +0100
@@ -16,7 +16,7 @@
  * 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
 
-#include "/var/lib/pcp/pmdas/linux/domain.h"
+#define LINUX 60
 
 /*************************
  * PMDA / CLIENT INTERFACE
--- pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/ib/domain.h	1970-01-01 01:00:00.000000000 +0100
+++ pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/ib/domain.h	2012-12-27 17:40:17.350902547 +0100
@@ -0,0 +1 @@
+#define IB	91
--- pcp-pmda-cluster-1.0.1/src/tmpdata.orig/pmdas/ib/pmns	1970-01-01 01:00:00.000000000 +0100
+++ pcp-pmda-cluster-1.0.1/src/tmpdata/pmdas/ib/pmns	2012-12-27 17:40:22.790902434 +0100
@@ -0,0 +1,78 @@
+
+infiniband {
+	hca
+	port
+	control
+}
+
+infiniband.hca {
+	type		IB:0:0
+	ca_type		IB:0:1
+	numports	IB:0:2
+	fw_ver		IB:0:3
+	hw_ver		IB:0:4
+	node_guid	IB:0:5
+	system_guid	IB:0:6
+}
+
+infiniband.port {
+	guid		IB:0:7
+	gid_prefix	IB:0:8
+	lid		IB:0:9
+	state		IB:0:10
+	phystate	IB:0:11
+	rate		IB:0:12
+	capabilities	IB:0:13
+	linkspeed	IB:0:14
+	linkwidth	IB:0:15
+
+	in
+	out
+	total
+}
+
+infiniband.port.in {
+	bytes		IB:1:0
+	packets		IB:1:1
+	errors
+}
+
+infiniband.port.in.errors {
+	drop		IB:1:4
+    	filter          IB:1:9
+    	local           IB:1:10
+    	remote		IB:1:11
+}
+
+infiniband.port.out {
+	bytes		IB:1:2
+	packets		IB:1:3
+	errors
+}
+
+infiniband.port.out.errors {
+	drop		IB:1:5
+	filter		IB:1:12
+}
+
+infiniband.port.total {
+	bytes		IB:1:6
+	packets		IB:1:7
+	errors
+}
+
+infiniband.port.total.errors {
+	drop		IB:1:8
+	filter          IB:1:13
+	link            IB:1:14
+	recover         IB:1:15
+	integrity       IB:1:16
+	vl15            IB:1:17
+	overrun         IB:1:18
+	symbol          IB:1:19
+}
+
+infiniband.control {
+	query_timeout	IB:2:0
+	hiwat		IB:2:1
+}
